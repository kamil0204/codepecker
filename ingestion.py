"""
Data Ingestion Tool - Codebase Analysis Pipeline

This script performs the complete data ingestion process:
1. Scans project directory structure
2. Uses LLM to identify entry points
3. Parses code files using tree-sitter
4. Stores parsed data in Neo4j graph database

Usage: python ingestion.py
Configure the target project path in the main() function.
"""
import os
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from src.utils.directory_scanner import get_directory_tree, generate_text_tree
from src.llm.llm_client import generate_entrypoints_list, review_csharp_methods
from src.parsers.parser_manager import ParserManager
from src.database.graph_db_factory import CallStackGraphDB
from src.core.config import Config


def parse_files_parallel(parser_manager, grouped_files, max_workers=4):
    """
    Parse files in parallel using ThreadPoolExecutor
    
    Args:
        parser_manager: ParserManager instance
        grouped_files: Dictionary of language -> list of file paths
        max_workers: Maximum number of threads to use
    
    Returns:
        Dictionary containing parsing results for all files
    """
    print(f"   ‚Ä¢ Using {max_workers} threads for parallel parsing")
    
    all_results = {}
    lock = threading.Lock()
    
    def parse_single_file(file_path, language):
        """Parse a single file and return results"""
        try:
            # Get the appropriate parser for the language
            if language in parser_manager.parsers:
                parser = parser_manager.parsers[language]
                result = parser.parse_file(file_path)
                return file_path, result
            else:
                print(f"   ‚ö†Ô∏è  No parser available for language: {language}")
                return file_path, None
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Error parsing {file_path}: {e}")
            return file_path, None
    
    # Process each language group
    for language, file_paths in grouped_files.items():
        print(f"   ‚Ä¢ Processing {len(file_paths)} {language} files...")
        
        language_results = {}
        
        # Use ThreadPoolExecutor for parallel processing
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all files for parsing
            future_to_file = {
                executor.submit(parse_single_file, file_path, language): file_path 
                for file_path in file_paths
            }
            
            # Collect results as they complete
            completed = 0
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    parsed_file, result = future.result()
                    if result:
                        with lock:
                            language_results[parsed_file] = result
                    completed += 1
                    
                    # Progress indicator
                    if completed % 10 == 0 or completed == len(file_paths):
                        print(f"   ‚Ä¢ Completed {completed}/{len(file_paths)} {language} files")
                        
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Failed to process {file_path}: {e}")
        
        all_results[language] = language_results
        print(f"   ‚úÖ Completed {language}: {len(language_results)} files successfully parsed")
    
    return all_results


def review_methods_with_llm_parallel(graph_db, batch_size=10, max_workers=3):
    """
    Review all methods in the database using LLM in parallel batches
    
    Args:
        graph_db: Graph database instance
        batch_size: Number of methods to review in each batch
        max_workers: Maximum number of parallel threads
    """
    print("üîç Step 6: Reviewing methods with LLM...")
    
    # Clear existing reviews first
    graph_db.clear_existing_reviews()
    print("   ‚Ä¢ Cleared existing reviews")
    
    # Get all methods that need review
    all_methods = graph_db.get_all_methods_for_review()
    total_methods = len(all_methods)
    
    if total_methods == 0:
        print("   ‚Ä¢ No methods found for review")
        return
    
    print(f"   ‚Ä¢ Found {total_methods} methods to review")
    print(f"   ‚Ä¢ Using batch size: {batch_size}, workers: {max_workers}")
    
    # Split methods into batches
    batches = [all_methods[i:i + batch_size] for i in range(0, total_methods, batch_size)]
    print(f"   ‚Ä¢ Split into {len(batches)} batches")
    
    total_reviews_created = 0
    total_batches_processed = 0
    
    def process_batch(batch_methods):
        """Process a batch of methods for review"""
        batch_reviews = 0
        
        for method in batch_methods:
            try:
                # Prepare method data for LLM review
                method_data = {
                    "name": method["method_name"],
                    "class_name": method["class_name"],
                    "visibility": method["visibility"],
                    "definition": method["definition"]
                }
                
                # Call LLM for review
                review_json = review_csharp_methods([method_data], method["file_path"])
                
                if review_json:
                    try:
                        # Clean the JSON response (remove markdown code blocks if present)
                        cleaned_review_json = review_json.strip()
                        if cleaned_review_json.startswith('```json'):
                            # Remove markdown code block wrapper
                            cleaned_review_json = cleaned_review_json[7:]  # Remove '```json'
                            if cleaned_review_json.endswith('```'):
                                cleaned_review_json = cleaned_review_json[:-3]  # Remove trailing '```'
                            cleaned_review_json = cleaned_review_json.strip()
                        elif cleaned_review_json.startswith('```'):
                            # Handle generic code blocks
                            cleaned_review_json = cleaned_review_json[3:]  # Remove '```'
                            if cleaned_review_json.endswith('```'):
                                cleaned_review_json = cleaned_review_json[:-3]  # Remove trailing '```'
                            cleaned_review_json = cleaned_review_json.strip()
                        
                        reviews = json.loads(cleaned_review_json)
                        
                        # Store each review in the database
                        for review in reviews:
                            if isinstance(review, dict) and review.get("method_name"):
                                graph_db.add_review(method["method_id"], review)
                                batch_reviews += 1
                                
                    except json.JSONDecodeError as e:
                        print(f"   ‚ö†Ô∏è  JSON decode error for method {method['class_name']}.{method['method_name']}: {e}")
                        
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Error reviewing method {method['class_name']}.{method['method_name']}: {e}")
        
        return batch_reviews
    
    # Process batches in parallel
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_batch = {
            executor.submit(process_batch, batch): i 
            for i, batch in enumerate(batches)
        }
        
        for future in as_completed(future_to_batch):
            batch_index = future_to_batch[future]
            try:
                batch_reviews = future.result()
                total_reviews_created += batch_reviews
                total_batches_processed += 1
                
                progress = (total_batches_processed / len(batches)) * 100
                print(f"   ‚Ä¢ Batch {batch_index + 1}/{len(batches)} completed ({progress:.1f}%) - {batch_reviews} reviews created")
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Batch {batch_index + 1} failed: {e}")
    
    print(f"‚úÖ Method review completed: {total_reviews_created} reviews created for {total_methods} methods")


def establish_method_call_relationships(graph_db, entry_point_results, remaining_results):
    """
    Establish relationships between method calls and their actual target methods
    
    Args:
        graph_db: Graph database instance
        entry_point_results: Parsing results from entry point files
        remaining_results: Parsing results from remaining files
    """
    print("   ‚Ä¢ Building method index from all parsed classes...")
    
    # Combine all parsing results
    all_results = {}
    for lang_results in [entry_point_results, remaining_results]:
        for language, file_results in lang_results.items():
            if language not in all_results:
                all_results[language] = {}
            all_results[language].update(file_results)
    
    # Build a comprehensive method index: method_name -> [(class_name, file_path, method_info)]
    method_index = {}
    class_method_map = {}  # class_name -> {method_name: method_info}
    
    for language, file_results in all_results.items():
        for file_path, file_result in file_results.items():
            if 'error' in file_result or 'classes' not in file_result:
                continue
                
            for class_info in file_result['classes']:
                class_name = class_info['name']
                class_method_map[class_name] = {}
                
                for method_info in class_info.get('methods', []):
                    method_name = method_info['name']
                    
                    # Add to method index
                    if method_name not in method_index:
                        method_index[method_name] = []
                    method_index[method_name].append((class_name, file_path, method_info))
                    
                    # Add to class method map
                    class_method_map[class_name][method_name] = method_info
    
    print(f"   ‚Ä¢ Indexed {len(method_index)} unique method names across {len(class_method_map)} classes")
    
    # Process method calls and establish relationships
    # relationships_created = 0
    
    # for language, file_results in all_results.items():
    #     for file_path, file_result in file_results.items():
    #         if 'error' in file_result or 'classes' not in file_result:
    #             continue
                
    #         for class_info in file_result['classes']:
    #             calling_class = class_info['name']
                
    #             for method_info in class_info.get('methods', []):
    #                 calling_method = method_info['name']
    #                 method_calls = method_info.get('method_calls', [])  # Changed from 'calls' to 'method_calls'
                    
    #                 for called_method_name in method_calls:
    #                     # Skip if calling itself (recursive calls are already handled)
    #                     if called_method_name == calling_method:
    #                         continue
                            
    #                     # Find potential target methods
    #                     target_methods = find_target_method(
    #                         called_method_name, 
    #                         calling_class, 
    #                         method_index, 
    #                         class_method_map
    #                     )
                        
    #                     # Create relationships for each target method found
    #                     for target_class, target_file, target_method_info in target_methods:
    #                         try:
    #                             graph_db.create_method_call_relationship(
    #                                 calling_class=calling_class,
    #                                 calling_method=calling_method,
    #                                 target_class=target_class,
    #                                 target_method=called_method_name,
    #                                 call_type="METHOD_CALL"
    #                             )
    #                             relationships_created += 1
                                
    #                             if relationships_created <= 10:  # Show first 10 for debugging
    #                                 print(f"   ‚Ä¢ Created: {calling_class}.{calling_method} -> {target_class}.{called_method_name}")
    #                             elif relationships_created == 11:
    #                                 print("   ‚Ä¢ (Additional relationships created...)")
                                    
    #                         except Exception as e:
    #                             print(f"   ‚ö†Ô∏è  Error creating relationship {calling_class}.{calling_method} -> {target_class}.{called_method_name}: {e}")
    
    # print(f"   ‚Ä¢ Total relationships created: {relationships_created}")


def find_target_method(called_method_name, calling_class, method_index, class_method_map):
    """
    Find potential target methods for a method call
    
    Args:
        called_method_name: Name of the called method
        calling_class: Name of the class making the call
        method_index: Dictionary of method_name -> [(class_name, file_path, method_info)]
        class_method_map: Dictionary of class_name -> {method_name: method_info}
    
    Returns:
        List of tuples: [(target_class, target_file, target_method_info)]
    """
    target_methods = []
    
    # Check if the method exists in the method index
    if called_method_name in method_index:
        potential_targets = method_index[called_method_name]
        
        # Filter out self-calls (same class)
        for class_name, file_path, method_info in potential_targets:
            if class_name != calling_class:  # Don't include calls within the same class
                target_methods.append((class_name, file_path, method_info))
    
    return target_methods


def main():
    """
    Main ingestion pipeline function
    
    Processes a codebase through the complete ingestion pipeline:
    1. Directory scanning and tree generation
    2. LLM-based entry point identification  
    3. Code parsing using tree-sitter
    4. Graph database storage and visualization
    """
    # Configure the target project path here
    project_path = "C:\\repos\\applications.web.intel-foundry.ifs3.api-project"
    
    print("="*60)
    print("CODEBASE INGESTION PIPELINE")
    print("="*60)
    print(f"üìÅ Target Project: {project_path}")
    print()
    
    if not os.path.exists(project_path):
        print(f"‚ùå Error: Path does not exist: {project_path}")
        return
    
    # Generate directory tree for LLM analysis (not saved to file)
    print("üìã Step 1: Scanning project directory structure...")
    tree_items = get_directory_tree(project_path)
    root_name = os.path.basename(project_path) or "Root"
    text_tree = generate_text_tree(tree_items, root_name)
    print(f"‚úÖ Found {len(tree_items)} items in project structure")
    
    # Generate AI entrypoints list
    print("\nüß† Step 2: Analyzing codebase structure with LLM...")
    entrypoints_json = generate_entrypoints_list(text_tree, project_path)
    
    if entrypoints_json:
        try:
            # Clean the JSON response (remove markdown code blocks if present)
            cleaned_json = entrypoints_json.strip()
            if cleaned_json.startswith('```json'):
                # Remove markdown code block wrapper
                cleaned_json = cleaned_json[7:]  # Remove '```json'
                if cleaned_json.endswith('```'):
                    cleaned_json = cleaned_json[:-3]  # Remove trailing '```'
                cleaned_json = cleaned_json.strip()
            elif cleaned_json.startswith('```'):
                # Handle generic code blocks
                cleaned_json = cleaned_json[3:]  # Remove '```'
                if cleaned_json.endswith('```'):
                    cleaned_json = cleaned_json[:-3]  # Remove trailing '```'
                cleaned_json = cleaned_json.strip()
            
            # Parse the JSON response
            entrypoints_list = json.loads(cleaned_json)
            
            # Display summary
            print(f"‚úÖ Identified {len(entrypoints_list)} entry points:")
            for entry in entrypoints_list:
                print(f"   ‚Ä¢ {entry['entrypoint']} ({entry['type']}) in {entry['file']}")
            
            # Initialize parser manager
            print("\nüîç Step 3: Parsing code files...")
            parser_manager = ParserManager()
            
            # Extract file paths from entrypoints
            file_paths = parser_manager.extract_files_from_entrypoints(entrypoints_list, project_path)
            print(f"   ‚Ä¢ Extracted {len(file_paths)} unique files from entrypoints")
            
            # Group files by language
            grouped_files = parser_manager.group_files_by_language(file_paths)
            print(f"   ‚Ä¢ Grouped files by language: {list(grouped_files.keys())}")
            
            # Parse files using appropriate parsers
            parsing_results = parser_manager.parse_files_by_language(grouped_files)
            print("‚úÖ Code parsing completed")
            
            # Create and populate graph database
            print("\nüìä Step 4: Storing data in graph database...")
            
            # Initialize graph database using configuration
            db_config = Config.get_database_config()
            graph_db = CallStackGraphDB(db_type=Config.DATABASE_TYPE, **db_config)
            print(f"   ‚Ä¢ Using {Config.DATABASE_TYPE.upper()} database")
            
            graph_db.store_parsing_results(parsing_results)
            print("   ‚Ä¢ Data ingestion completed")
            
            # Additional step: Parse remaining files not identified as entry points
            print("\nüîÑ Step 5: Processing remaining files with parallelized parsing...")
            
            # # Get all C# files from the directory tree
            # all_cs_files = []
            # for item in tree_items:
            #     # tree_items contains tuples: (item_type, name, level, full_path)
            #     item_type, name, level, file_path = item
            #     if item_type == 'file' and file_path.endswith('.cs') and os.path.isfile(file_path):
            #         all_cs_files.append(file_path)
            
            # # Get entry point files for comparison
            # entry_point_files = set(file_paths)  # These were already processed
            
            # Filter out entry point files to get remaining files
            # remaining_files = [f for f in all_cs_files if f not in entry_point_files]
            
            # print(f"   ‚Ä¢ Found {len(all_cs_files)} total C# files")
            # print(f"   ‚Ä¢ Already processed {len(entry_point_files)} entry point files")
            # print(f"   ‚Ä¢ Remaining {len(remaining_files)} files to process")
            
            # if remaining_files:
            #     # Group remaining files by language (mostly C# in this case)
            #     remaining_grouped = parser_manager.group_files_by_language(remaining_files)
            #     print(f"   ‚Ä¢ Grouped remaining files by language: {list(remaining_grouped.keys())}")
                
            #     # Parse remaining files with parallelization
            #     print("   ‚Ä¢ Starting parallelized parsing of remaining files...")
            #     remaining_parsing_results = parse_files_parallel(parser_manager, remaining_grouped)
            #     print("‚úÖ Parallelized parsing completed")
                
            #     # Store additional parsing results in the database
            #     print("   ‚Ä¢ Storing additional parsing results...")
            #     graph_db.store_parsing_results(remaining_parsing_results)
            #     print("   ‚Ä¢ Additional data ingestion completed")
            # else:
            #     print("   ‚Ä¢ No additional files to process")

            # Additional step: Establish method call relationships
            # print("\nüîó Step 6: Establishing method call relationships...")
            # establish_method_call_relationships(graph_db, parsing_results, remaining_parsing_results if remaining_files else {})
            # print("‚úÖ Method call relationships established")

            # Review methods using LLM
            review_methods_with_llm_parallel(graph_db, batch_size=5, max_workers=3)

            print(f"\nüìà Generated Code Structure Visualization:")
            print("="*60)
            graph_db.print_graph()
            graph_db.close()
            
            print("="*60)
            print("‚úÖ INGESTION PIPELINE COMPLETED SUCCESSFULLY")
            print("="*60)
            print(f"üìä Data stored in {Config.DATABASE_TYPE.upper()} database")
            if Config.DATABASE_TYPE == "neo4j":
                print("üåê View in Neo4j Browser: http://localhost:7474")
            print()
            
        except json.JSONDecodeError as e:
            print(f"‚ùå Failed to parse JSON response: {e}")
            print(f"Raw response: {entrypoints_json}")
    else:
        print("‚ùå Entrypoints list generation failed. Check your API credentials and network connection.")


if __name__ == "__main__":
    main()
